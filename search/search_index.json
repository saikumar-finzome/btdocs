{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Backtify","text":""},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#dockerfile","title":"Dockerfile","text":"<p>Now in the same project directory create a file <code>Dockerfile</code> with:</p> <pre><code># (1)\nFROM python:3.9\n# (2)\nWORKDIR /code\n# (3)\nCOPY ./requirements.txt /code/requirements.txt\n\n# (4)\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\n# (5)\nCOPY ./app /code/app\n\n# (6)\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <ol> <li> <p>Start from the official Python base image.</p> </li> <li> <p>Set the current working directory to <code>/code</code>.</p> <p>This is where we'll put the <code>requirements.txt</code> file and the <code>app</code> directory.</p> </li> <li> <p>Copy the file with the requirements to the <code>/code</code> directory.</p> <p>Copy only the file with the requirements first, not the rest of the code.</p> <p>As this file doesn't change often, Docker will detect it and use the cache for this step, enabling the cache for the next step too.</p> </li> <li> <p>Install the package dependencies in the requirements file.</p> <p>The <code>--no-cache-dir</code> option tells <code>pip</code> to not save the downloaded packages locally, as that is only if <code>pip</code> was going to be run again to install the same packages, but that's not the case when working with containers.</p> <p>Note</p> <p>The <code>--no-cache-dir</code> is only related to <code>pip</code>, it has nothing to do with Docker or containers.</p> <p>The <code>--upgrade</code> option tells <code>pip</code> to upgrade the packages if they are already installed.</p> <p>Because the previous step copying the file could be detected by the Docker cache, this step will also use the Docker cache when available.</p> <p>Using the cache in this step will save you a lot of time when building the image again and again during development, instead of downloading and installing all the dependencies every time.</p> </li> <li> <p>Copy the <code>./app</code> directory inside the <code>/code</code> directory.</p> <p>As this has all the code which is what changes most frequently the Docker cache won't be used for this or any following steps easily.</p> <p>So, it's important to put this near the end of the <code>Dockerfile</code>, to optimize the container image build times.</p> </li> <li> <p>Set the command to run the <code>uvicorn</code> server.</p> <p><code>CMD</code> takes a list of strings, each of these strings is what you would type in the command line separated by spaces.</p> <p>This command will be run from the current working directory, the same <code>/code</code> directory you set above with <code>WORKDIR /code</code>.</p> <p>Because the program will be started at <code>/code</code> and inside of it is the directory <code>./app</code> with your code, Uvicorn will be able to see and import <code>app</code> from <code>app.main</code>.</p> </li> </ol> <p>Tip</p> <p>Review what each line does by clicking each number bubble in the code. \ud83d\udc46</p> <p>You should now have a directory structure like:</p> <pre><code>.\n\u251c\u2500\u2500 app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"Dockers/","title":"Dockers","text":""},{"location":"Dockers/#list-of-docker-commands","title":"List of Docker Commands","text":""},{"location":"Dockers/#docker-container-management-commands","title":"Docker Container Management Commands","text":"<p>This section features the essential commands related to the lifecycle of Docker containers. Learn how to create, manage, and remove containers from your Docker system using the below commands. </p> <p>See the containers currently running on the system:</p> <pre><code>docker ps\n</code></pre> <p>See all the containers, both running and non-running:</p> <pre><code>docker ps -a\n</code></pre> <p>Create a container (without starting it):</p> <pre><code>docker create [image]\n</code></pre> <p>Create an interactive container with pseudo-TTY:</p> <pre><code>docker create -it [image]\n</code></pre> <p>Rename an existing container:</p> <pre><code>docker rename [container] [new-name]\n</code></pre> <p>Delete a container (if it is not running):</p> <pre><code>docker rm [container]\n</code></pre> <p>Forcefully remove a container, even if it is running:</p> <pre><code>docker rm -f [container]\n</code></pre> <p>View logs for a running container:</p> <pre><code>docker logs [container]\n</code></pre> <p>Retrieve logs created before a specific point in time:</p> <pre><code>docker logs -f --until=[interval] [container]\n</code></pre> <p>View real-time events for a container:</p> <pre><code>docker events [container]\n</code></pre> <p>Update the configuration of one or more containers:</p> <pre><code>docker update [container]\n</code></pre> <p>View port mapping for a container:</p> <pre><code>docker port [container]\n</code></pre> <p>Show running processes in a container:</p> <pre><code>docker top [container]\n</code></pre> <p>View live resource usage statistics for a container:</p> <pre><code>docker stats [container]\n</code></pre> <p>Show changes to files or directories on the filesystem:</p> <pre><code>docker diff [container]\n</code></pre> <p>Copy a local file to a directory in a container:</p> <pre><code>docker cp [file-path] CONTAINER:[path]\n</code></pre>"},{"location":"Dockers/#running-a-container","title":"Running a Container","text":"<p>The following commands show you how to start and stop processes in a container and how to manage container execution. </p> <p>Run a command in a container based on an image:</p> <pre><code>docker run [image] [command]\n</code></pre> <p>Create, start, and provide a custom name for the container:</p> <pre><code>docker run --name [container-name] [image]\n</code></pre> <p>Establish a connection with a container by mapping a host port to a container port:</p> <pre><code>docker run -p [host-port]:[container-port] [image]\n</code></pre> <p>Run a container and remove it after it stops:</p> <pre><code>docker run --rm [image]\n</code></pre> <p>Run a detached (background) container:</p> <pre><code>docker run -d [image]\n</code></pre> <p>Start an interactive process, such as a shell, in a container:</p> <pre><code>docker run -it [image]\n</code></pre> <p>Start a container:</p> <pre><code>docker start [container]\n</code></pre> <p>Stop a running container:</p> <pre><code>docker stop [container]\n</code></pre> <p>Stop a running container and start it up again:</p> <pre><code>docker restart [container]\n</code></pre> <p>Pause processes in a running container:</p> <pre><code>docker pause [container]\n</code></pre> <p>Resume processes in a running container:</p> <pre><code>docker unpause [container]\n</code></pre> <p>Block a container until others stop (after which it prints their exit codes):</p> <pre><code>docker wait [container]\n</code></pre> <p>Kill a container by sending a SIGKILL to a running container:</p> <pre><code>docker kill [container]\n</code></pre> <p>Attach local standard input, output, and error streams to a running container:</p> <pre><code>docker attach [container]\n</code></pre> <p>Run a shell inside a running container:</p> <pre><code>docker exec -it [container] [shell]\n</code></pre>"},{"location":"Dockers/#docker-image-commands","title":"Docker Image Commands","text":"<p>Below you will find all the necessary commands for working with Docker images. </p> <p>Create an image from a Dockerfile:</p> <pre><code>docker build [dockerfile-path]\n</code></pre> <p>Build an image from a Dockerfile located in the current directory:</p> <pre><code>docker build .\n</code></pre> <p>Create an image from a Dockerfile and tag it.</p> <pre><code>docker build -t [name]:[tag] [dockerfile-path]\n</code></pre> <p>Specify a file to build from:</p> <pre><code>docker build -f [file-path]\n</code></pre> <p>Pull an image from a registry:</p> <pre><code>docker pull [image]\n</code></pre> <p>Push an image to a registry:</p> <pre><code>docker push [image]\n</code></pre> <p>Create an image from a tarball:</p> <pre><code>docker import [url/file]\n</code></pre> <p>Create an image from a container:</p> <pre><code>docker commit [container] [new-image]\n</code></pre> <p>Tag an image:</p> <pre><code>docker tag [image] [image]:[tag]\n</code></pre> <p>Show all locally stored top-level images:</p> <pre><code>docker images\n</code></pre> <p>Show history for an image:</p> <pre><code>docker history [image]\n</code></pre> <p>Remove an image:</p> <pre><code>docker rmi [image]\n</code></pre> <p>Load an image from a tar archive or stdin:</p> <pre><code>docker load --image [tar-file]\n</code></pre> <p>Save an image to a tar archive file:</p> <pre><code>docker save [image] &gt; [tar-file]\n</code></pre> <p>Remove unused images:</p> <pre><code>docker image prune\n</code></pre>"},{"location":"Dockers/#docker-networking-commands","title":"Docker Networking Commands","text":"<p>One of the most valuable features of Docker software is the ability to connect containers to each other and to other non-Docker workloads. This section covers network-related commands. </p> <p>List available networks:</p> <pre><code>docker network ls\n</code></pre> <p>Remove one or more networks:</p> <pre><code>docker network rm [network]\n</code></pre> <p>Show information on one or more networks:</p> <pre><code>docker network inspect [network]\n</code></pre> <p>Connect a container to a network:</p> <pre><code>docker network connect [network] [container]\n</code></pre> <p>Disconnect a container from a network:</p> <pre><code>docker network disconnect [network] [container]\n</code></pre>"},{"location":"Dockers/#docker-general-management-commands","title":"Docker General Management Commands","text":"<p>Once you set up your containers, you will need to know how to get all the important information for managing them. The following commands will allow you to obtain general information about the system and connect to remote Docker resources. </p> <p>Log in to a Docker registry:</p> <pre><code>docker login\n</code></pre> <p>Log out of a Docker registry:</p> <pre><code>docker logout\n</code></pre> <p>List low-level information on Docker objects:</p> <pre><code>docker inspect [object]\n</code></pre> <p>Show the version of the local Docker installation:</p> <pre><code>docker version\n</code></pre> <p>Display information about the system:</p> <pre><code>docker info\n</code></pre> <p>Remove unused images, containers, and networks:</p> <pre><code>docker system prune\n</code></pre>"},{"location":"Dockers/#plugin-management-commands","title":"Plugin Management Commands","text":"<p>Docker plugins extend Docker's functionality and help users connect with other popular services. The commands below let you add, manage, and remove plugins on your system. </p> <p>Enable a Docker plugin:</p> <pre><code>docker plugin enable [plugin]\n</code></pre> <p>Disable a Docker plugin:</p> <pre><code>docker plugin disable [plugin]\n</code></pre> <p>Create a plugin from config.json and rootfs:</p> <pre><code>docker plugin create [plugin] [path-to-data]\n</code></pre> <p>View details about a plugin:</p> <pre><code>docker plugin inspect [plugin]\n</code></pre> <p>Remove a plugin:</p> <pre><code>docker plugin rm [plugin]\n</code></pre>"},{"location":"nginx/","title":"Nginx","text":"<p>For blog visit dev.to.</p>"},{"location":"nginx/#step-by-step-implementation","title":"Step by step Implementation :","text":""},{"location":"nginx/#step-1-local-setup","title":"Step 1: LOCAL SETUP","text":"<p>We start by creating a local workspace on our machine so navigate to desktop and create a folder hello_fastapi and open it using your code editor. Next let's create a virtual environment to hold our dependencies.</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>For Linux</p> <pre><code>Virtualenv env\n</code></pre> <p>For Windows</p> <pre><code>python -m venv env\n</code></pre> <p>Then activate it using</p> <pre><code>env/Scripts/activate on windows\nsource env/bin/activate on linux/unix\n</code></pre> <p>Now that we have our environment set up let's install what we need.</p> <pre><code>pip install fastapi uvicorn gunicorn psycopg2-binary\n</code></pre> <p>You may add the deps depending on what your project will need in order to work. With the deps installed we then create a file that will hold them all.</p> <pre><code>pip install fastapi uvicorn gunicorn psycopg2-binary\n</code></pre> <p>Next create a file called main.py at the root of the project and add the following code to it.</p> <pre><code>import uvicorn\nif __name__ == \"__main__\":\nuvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=True)\n</code></pre> <p>Then create another file at the root called app.py with the following code:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\napp = FastAPI()\norigins = [\"*\"]\napp.add_middleware(\nCORSMiddleware,\nallow_origins=origins,\nallow_credentials=True,\nallow_methods=[\"*\"],\nallow_headers=[\"*\"],\n)\n@app.get(\"/\", tags=[\"Root\"])\nasync def read_root():\nreturn {\"message\": \"Welcome to the API!\"}\n</code></pre> <p>Let's test the app to see if it is running well on our local browser: Run the server by:</p> <pre><code>uvicorn  app:app --reload\n</code></pre>"},{"location":"nginx/#step-2-version-control","title":"Step 2: VERSION CONTROL","text":"<p>While still in the root of the project we can issue the following commands to push our code to GitHub:</p> <pre><code>git init\ngit add .\ngit commit -m 'initial'\ngit remote add origin REMOTE URL HERE\ngit push -u origin master\n</code></pre>"},{"location":"nginx/#step-3-deploying-to-aws-ec2","title":"Step 3: DEPLOYING TO AWS EC2","text":"<p>The assumption is making here is that you have an AWS account and have created a free tier EC2 server running on Ubuntu. Feel free to check out docs on how to do that here. These steps will work on any Ubuntu machine though so if you are on DigitalOcean, Linode or Oracle it should work well. Let's ssh into our remote machine then:</p> <pre><code>ssh -i 'example.pem' ubuntu@ec2-IP-ADDRESS.us-east-2.compute.amazonaws.com\n</code></pre> <p>Once you are inside the remote machine we need to install Python and NGINX. Issue the following commands on the terminal:</p> <pre><code>sudo apt update\nsudo apt install python3-pip python3-dev libpq-dev postgresql postgresql-contrib nginx curl\nsudo -H pip3 install --upgrade pip\n</code></pre> <p>Next create a folder that will carry our files: <pre><code>mkdir apiv1 &amp;&amp; cd apiv1\n</code></pre> Then create and activate a virtual environment like we did before. After that initiate git inside the folder and pull your code from gitHub.</p> <p><pre><code>git init\ngit remote add origin REPO URL\ngit pull origin master\n</code></pre> Now we install our deps like so:</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> Deactivate the virtualenv after running the local server and ensuring no errors are reported. Next up we need to bind our application to a gunicorn module that serves as an interface to your application, translating client requests from HTTP to Python calls that your application can process. So we start by creating a socket file:</p> <pre><code>sudo nano /etc/systemd/system/gunicorn.socket\n</code></pre> <p>Add the following code to it: <pre><code>[Unit]\nDescription=gunicorn socket\n\n[Socket]\nListenStream=/run/gunicorn.sock\n\n[Install]\nWantedBy=sockets.target\n</code></pre> Save and close that file. Next we create a service file: <pre><code>sudo nano /etc/systemd/system/gunicorn.service\n</code></pre></p> <p>Add the following code to it:</p> <p><pre><code># (1)\n[Unit]\nDescription=gunicorn daemon\nRequires=gunicorn.socket\nAfter=network.target\n[Service]\nUser=ubuntu\nGroup=www-data\nWorkingDirectory=/home/ubuntu/apiv1\nExecStart=/home/ubuntu/apiv1/env/bin/gunicorn \\\n--access-logfile - \\\n--workers 5 \\\n--bind unix:/run/gunicorn.sock \\\n--worker-class uvicorn.workers.UvicornWorker \\\napp:app\n[Install]\nWantedBy=multi-user.target\n</code></pre> test.1. :</p> <p>Save and close as well. Next start the Gunicorn socket:</p> <p><pre><code>sudo systemctl start gunicorn.socket\n</code></pre> Then enable it by:</p> <pre><code>sudo systemctl enable gunicorn.socket\n</code></pre> <p>Now that Gunicorn is set up, next we\u2019ll configure Nginx to pass traffic to the process. Start by creating and opening a new server block in Nginx\u2019s sites-available directory:</p> <pre><code>sudo nano /etc/nginx/sites-enabled/api\n</code></pre> <p>Add the following lines to it: <pre><code>server {\n    listen 80;\n    server_name server_domain_or_IP;\n    location / {\n        proxy_pass http://unix:/run/gunicorn.sock;\n    }\n}\n</code></pre> Save this file as well. Test that the config is okay by:</p> <p><pre><code>sudo nginx -t\n</code></pre> If all is well then we need to restart the services so go ahead and:</p> <p><pre><code>sudo systemctl daemon-reload\nsudo systemctl restart gunicorn\nsudo systemctl restart nginx\n</code></pre> One last step is to ensure that you have allowed HTTP access via port 80 on your instance's security group section. Do the same for port 443 if your app is served over HTTPS. If all the above is done then go to your instance's public IP address or domain, and hopefully you will see your app being displayed there.</p>"}]}